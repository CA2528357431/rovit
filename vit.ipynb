{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b796829-311e-4145-83f0-064c6c94ba10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "import logging\n",
    "\n",
    "# device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bbcf92e-0e43-4c76-a554-4dbf8817935c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log \n",
    "# record the process of training\n",
    "logging.basicConfig(\n",
    "    filename='vit.log',  \n",
    "    level=logging.INFO,         \n",
    "    format='%(asctime)s %(message)s',  \n",
    ")\n",
    "logger_all = logging.getLogger(\"logger_all\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d267631d-4a7e-430a-a4f4-34e79afc0131",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# make single item into a tuple of 2 same items\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "# FeedForward network with two fully connected layers and GELU activation\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Multi-head self-attention\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Dimension of concatenated attention heads\n",
    "        inner_dim = dim_head *  heads\n",
    "        \n",
    "        # whether the dimension of the attention's output is same to the expected dimension\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.dim_head = dim_head\n",
    "        \n",
    "        # qkv scaling\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "        # get attention weight\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # compute qkv at one time\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "        \n",
    "        # transfer the dimension of attention's output into expected output dimension\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        # get q, k, v\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "        \n",
    "        # get attention weight\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "        attn = self.attend(dots)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        # compute attention out put\n",
    "        out = torch.matmul(attn, v)\n",
    "        \n",
    "        # reshape \n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        \n",
    "        # change dimension\n",
    "        return self.to_out(out)\n",
    "\n",
    "# Original Transformer\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        \n",
    "        # several layers of rotational attention\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout),\n",
    "                FeedForward(dim, mlp_dim, dropout = dropout)\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "\n",
    "        return self.norm(x)\n",
    "\n",
    "# original Vision Transformer\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "        \n",
    "        # layers for transferring patches into token\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
    "            nn.LayerNorm(patch_dim),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "            nn.LayerNorm(dim),\n",
    "        )\n",
    "\n",
    "        # positional embedding require gradient\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "\n",
    "        # initial class token require gradient\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "        \n",
    "        # original transformer\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Linear(dim, num_classes)\n",
    "\n",
    "    def forward(self, img):\n",
    "        \n",
    "        # Embed image patches into a sequence of tokens\n",
    "        x = self.to_patch_embedding(img)\n",
    "        \n",
    "        b, n, _ = x.shape\n",
    "        \n",
    "        # add class token\n",
    "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b = b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        \n",
    "        # add positional embedding\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
    "\n",
    "        x = self.to_latent(x)\n",
    "        \n",
    "        # get classification results\n",
    "        return self.mlp_head(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e894f32a-6bbc-415e-8c7a-9cab45a97884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_rovit = RoViT(\n",
    "#     image_size = 256,\n",
    "#     patch_size = 16,\n",
    "#     num_classes = 100,\n",
    "#     dim = 512,\n",
    "#     depth = 6,\n",
    "#     heads = 16,\n",
    "#     mlp_dim = 1024,\n",
    "#     dropout = 0.1,\n",
    "#     emb_dropout = 0.1\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72405550-bbb8-40d6-905b-77d67fff4a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = torch.randn(8,3,256,256)\n",
    "# b = my_rovit(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74395c76-2dcd-4233-99e1-911ec1a5a0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# reshape image size and transform image into tensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),  \n",
    "])\n",
    "\n",
    "# train dataset and dataloader\n",
    "# use CIFAR-100\n",
    "train_batch = 128\n",
    "trainset = torchvision.datasets.CIFAR100(root=\"./cifar-100\",\n",
    "                                         train=True,\n",
    "                                         download=True,\n",
    "                                         transform=transform)\n",
    "# num_worker should be same to the number of CPU core\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, \n",
    "    batch_size=train_batch, \n",
    "    shuffle=True, \n",
    "    num_workers=16)\n",
    "\n",
    "\n",
    "# test dataset and dataloader\n",
    "# use CIFAR-100\n",
    "test_batch = 256\n",
    "testset = torchvision.datasets.CIFAR100(root=\"./cifar-100\",\n",
    "                                        train=False,\n",
    "                                        download=True,\n",
    "                                        transform=transform)\n",
    "# num_worker should be same to the number of CPU core\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, \n",
    "    batch_size=test_batch, \n",
    "    shuffle=False, \n",
    "    num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55f73bd-5d8a-45d7-b2e1-87b3c364e408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original model\n",
    "my_vit = ViT(\n",
    "    image_size = 256,\n",
    "    patch_size = 16,\n",
    "    num_classes = 100,\n",
    "    dim = 512,\n",
    "    depth = 6,\n",
    "    heads = 16,\n",
    "    mlp_dim = 1024,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03de8bd2-99b2-4426-a16e-af75fdfe5ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# folder_path = './ckpt/vit/'\n",
    "\n",
    "# file_names = os.listdir(folder_path)\n",
    "# file_names = [\"rovit_last.pth\"]+file_names\n",
    "# files = [x for x in file_names if x[-4:]==\".pth\" and \"last\" not in x]\n",
    "# cur_epoch = len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359e7dbb-7e48-44dd-b3f5-5357164e50c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Loss function for classification\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "lr = 5e-4\n",
    "optimizer = optim.Adam(my_vit.parameters(), lr=lr)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    # Set model to training mode\n",
    "    my_vit.train() \n",
    "    \n",
    "    # running loss for each epoch\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # counter of processed items for each epoch \n",
    "    cnt = 0\n",
    "    \n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        \n",
    "        # Move data to GPU\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # classification result\n",
    "        outputs = my_vit(images)\n",
    "        \n",
    "        # loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # optimize model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # count running loss and processed items\n",
    "        running_loss += loss.item()\n",
    "        b = images.shape[0]\n",
    "        cnt += b\n",
    "        \n",
    "        # log message for each iteration\n",
    "        msg = f'Epoch [{epoch + 1}/{num_epochs}], Item [{cnt}/{len(trainset)}], Loss: {running_loss / (i+1):.4f}'\n",
    "        logger_all.info(msg)\n",
    "        print(msg)\n",
    "    \n",
    "\n",
    "    # log message for each epoch\n",
    "    msg = f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(trainloader):.4f}'\n",
    "    logger_all.info(msg)\n",
    "    print(msg)\n",
    "    \n",
    "    # save checkpoint\n",
    "    if (epoch + 1)%5==0:\n",
    "        model_save_path = f'./ckpt/vit/vit_{epoch+1}.pth'\n",
    "        torch.save(my_vit.state_dict(), model_save_path)\n",
    "        print(f'Model parameters saved to {model_save_path}')\n",
    "\n",
    "model_save_path = f'./ckpt/vit/vit_last.pth'\n",
    "torch.save(my_vit.state_dict(), model_save_path)\n",
    "print(f'Model parameters saved to {model_save_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a0ba7e-a8c4-413d-b32b-8b3373418093",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# my_vit.eval()  \n",
    "# correct = 0\n",
    "# total = 0\n",
    "# \n",
    "# with torch.no_grad():\n",
    "#     for images, labels in testloader:\n",
    "#         images = images.to(device)\n",
    "#         labels = labels.to(device)\n",
    "#         outputs = my_vit(images)\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "# \n",
    "# accuracy = 100 * correct / total\n",
    "# print(f'Accuracy on CIFAR-100 test images: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264ff3a2-840c-496c-a4b0-269fcd7f569d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a663c05-1913-4076-8443-d5963b6e113e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_vit = ViT(\n",
    "#     image_size = 256,\n",
    "#     patch_size = 16,\n",
    "#     num_classes = 100,\n",
    "#     dim = 512,\n",
    "#     depth = 6,\n",
    "#     heads = 16,\n",
    "#     mlp_dim = 1024,\n",
    "#     dropout = 0.1,\n",
    "#     emb_dropout = 0.1\n",
    "# ).to(device)\n",
    "# model_save_path = './ckpt/vit/vit_50.pth'\n",
    "# my_vit.load_state_dict(torch.load(model_save_path))\n",
    "# my_vit = my_vit.to(device)\n",
    "# my_vit.eval()  \n",
    "# print(\"Model loaded and ready for inference.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8843e3-a53b-46a8-b361-c139b328e1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = torch.randn(8,3,256,256).to(device)\n",
    "# b = my_vit(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21779018-2037-47e6-b681-9c38740a2a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_vit.eval()  \n",
    "# correct = 0\n",
    "# total = 0\n",
    "# \n",
    "# with torch.no_grad():\n",
    "#     for images, labels in testloader:\n",
    "#         images = images.to(device)\n",
    "#         labels = labels.to(device)\n",
    "#         outputs = my_vit(images)\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "# \n",
    "# accuracy = 100 * correct / total\n",
    "# print(f'Accuracy on CIFAR-100 test images: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b961d15a-dda8-47d2-a5a6-96bfda390d34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b16d203-4ec3-4a15-9c13-2805e282984a",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17983ee7-deaf-4a07-be19-13feea5c0df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation for models at different epoches\n",
    "for e in range(5,55,5):\n",
    "    my_vit = ViT(\n",
    "    image_size = 256,\n",
    "    patch_size = 16,\n",
    "    num_classes = 100,\n",
    "    dim = 512,\n",
    "    depth = 6,\n",
    "    heads = 16,\n",
    "    mlp_dim = 1024,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1\n",
    ").to(device)\n",
    "    \n",
    "    # load checkpoint\n",
    "    model_save_path = f'./ckpt/vit/vit_{e}.pth'\n",
    "    my_vit.load_state_dict(torch.load(model_save_path))\n",
    "    \n",
    "    # mode model to GPU\n",
    "    my_vit = my_vit.to(device)\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    my_vit.eval() \n",
    "\n",
    "    # Accuracy calculation\n",
    "    # counter for test\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            \n",
    "            # Move data to GPU\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # classification results\n",
    "            outputs = my_vit(images)\n",
    "            \n",
    "            # predicted class label\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            # update counters\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    # get test result for an epoch\n",
    "    accuracy = 100 * correct / total\n",
    "    res.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d0a574b-31fa-4f8e-bc67-1d96ba7805af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28.53, 33.39, 34.52, 36.76, 37.55, 37.77, 38.59, 37.21, 38.02, 38.76, 38.14, 39.11, 39.58, 39.0, 38.45, 38.9, 38.91, 39.15, 38.67, 39.36, 39.03, 39.61, 40.27, 40.07, 39.9, 39.98, 39.8, 40.08, 40.68, 39.94]\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b71802-46d1-4e60-8c72-6c89f4014e0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78f6e1e-7d2d-4d7c-bb1c-3133ce80ebe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # 指定文件夹路径\n",
    "# directory = 'ckpt/vit'\n",
    "\n",
    "# # 列出目录中的所有文件\n",
    "# for filename in os.listdir(directory):\n",
    "#     # 检查文件是否以 .pth 结尾\n",
    "#     if filename.endswith(\".pth\"):\n",
    "#         if not filename.endswith('5.pth') and not filename.endswith('0.pth'):\n",
    "#                 file_path = os.path.join(directory, filename)\n",
    "#                 # 删除文件\n",
    "#                 os.remove(file_path)\n",
    "#                 print(f\"Deleted: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed48e49f-43c3-49bb-8212-c4a6b2c25b09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
